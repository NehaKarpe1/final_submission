{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf1a3219",
   "metadata": {},
   "source": [
    "**Project Title:** End-to-End Data Engineering Pipeline using Microsoft Fabric with Medallion Architecture\n",
    "By -Neha Karpe \n",
    "INT-19\n",
    "\n",
    "**Objective:**\n",
    "To implement an automated ETL pipeline using Microsoft Fabric, following the Medallion Architecture (Bronze, Silver, Gold). This includes data ingestion, cleaning, transformation, aggregation, logging, and visualization using Power BI.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1. **Ingesting Data into Bronze Layer**\n",
    "- **Method Used:** Copy Data Activity in Fabric pipeline\n",
    "- **Source:** On-premise CSV files (e.g., `region_dim.csv`, `product_dim.csv`, `sales_fact.csv`)\n",
    "-- **Access Method: Connected to the on-premise data source using Microsoft On-premises Data Gateway to securely access local files from within the cloud pipeline. This ensured seamless and secure data movement between on-premise file system and Fabric Lakehouse.\n",
    "- **Destination:** Bronze Lakehouse (Files section)\n",
    "- **Logging:**\n",
    "  - Source filename\n",
    "  - Ingestion timestamp\n",
    "  - Row count\n",
    "  - File format\n",
    "  - Any ingestion error messages\n",
    "\n",
    "**Good Practice Followed:**\n",
    "- Understood and verified the column names during the initial step to prevent transformation errors downstream.\n",
    "- Logged metadata at the point of ingestion.\n",
    "\n",
    "#### 2. **Bronze to Bronze Table Conversion**\n",
    "- **Tool Used:** Fabric Notebook (PySpark)\n",
    "- **Operation:**\n",
    "  - Renamed columns for consistency.\n",
    "  - Wrote data into Bronze Delta Tables.\n",
    "  - Ensured schema correctness.\n",
    "\n",
    "```python\n",
    "# Example\n",
    "sales_df = (spark.read.option(\"header\", True)\n",
    "            .csv(\"/lakehouse/Files/sales_fact.csv\")\n",
    "            .withColumnRenamed(\"Qty\", \"TotalQuantity\"))\n",
    "sales_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"sales_fact\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Transforming Data in Silver Layer**  \n",
    "- **Tool Used:** `Notebook26102002#silvernb.ipynb`\n",
    "- **Operations:**\n",
    "  - Read Bronze Delta Tables\n",
    "  - Applied transformations: data type casting, null filtering, schema alignment\n",
    "  - Logged:\n",
    "    - Data Quality issues (nulls, mismatches)\n",
    "    - Columns changed or removed\n",
    "    - Schema before and after transformation\n",
    "\n",
    "```python\n",
    "# Example Log\n",
    "silver_log = {\n",
    "    \"table\": \"sales_fact\",\n",
    "    \"records_dropped\": 12,\n",
    "    \"columns_converted\": [\"TotalCost: string -> double\"],\n",
    "    \"transformation_time\": current_timestamp()\n",
    "}\n",
    "```\n",
    "\n",
    "**Best Practices Applied:**\n",
    "- Maintained transformation logs\n",
    "- Used modular functions with docstrings for reuse and readability\n",
    "- Followed standard naming conventions\n",
    "\n",
    "\n",
    "#### 4. **Aggregating and Loading Gold Layer (Star Schema)**\n",
    "- **Tool Used:** `Notebook26102002#gold.ipynb`\n",
    "- **Steps:**\n",
    "  - Read from Silver\n",
    "  - Created dimension tables (Product, Region)\n",
    "  - Created fact tables with aggregations:\n",
    "    - Total Sales by Region and Category\n",
    "    - Top 5 Products by Sales Amount\n",
    "    - Monthly Sales Trend\n",
    "  - Logged aggregation steps, filters used, success/failure status\n",
    "\n",
    "\n",
    "\n",
    "**Blockers:**\n",
    "- 1)Warehouse table load failed due to unavailability of the assigned warehouse (documented and communicated)\n",
    "  2)Unable to access Power BI via fabric so couldnt complete the audit logs of accessing the gold layer lakehouse.\n",
    "\n",
    "\n",
    "#### 5. **Power BI Dashboard Creation**\n",
    "- Used Gold layer tables\n",
    "-KPI's\n",
    "Total Profit: 10.45M\n",
    "Total Sales: 29.36M\n",
    "Total Quantity: 54K\n",
    "- Built the following visuals:\n",
    "  - KPIs: Total Sales, Quantity, Profit\n",
    "  -Bar chart: Total Sales by Region\n",
    "  -Pie chart: Total Sales by Category\n",
    "  -Donut chart: Top Products by Sales\n",
    "  -Line chart: Monthly Sales Trend (2010â€“2014)\n",
    "- Made interactive using slicers (Region, Category, Date)\n",
    "-Region\n",
    "-Category\n",
    "-Year & Month (slicer)\n",
    "\n",
    "### ETL Terminologies Used:\n",
    "- **ETL (Extract, Transform, Load):** End-to-end pipeline covering raw ingestion to BI\n",
    "- **Delta Tables:** Used for versioned storage and optimized queries\n",
    "- **Star Schema:** Dimensional modeling in Gold layer\n",
    "- **Data Quality Checks:** Tracked nulls, types, and integrity\n",
    "- **Audit Logging:** Every stage included logs for traceability\n",
    "\n",
    "### Final Thoughts & Key Learning\n",
    "- Understood the significance of metadata logging and modular design\n",
    "- Practiced column verification as a good habit in the initial data load\n",
    "- Built modular, maintainable code using functions and comments\n",
    "- Gained exposure to Medallion Architecture and Power BI\n",
    "- Learned how to debug load issues and create visual stories from data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a536c59",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
